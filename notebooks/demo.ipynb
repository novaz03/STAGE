{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "#import contextily as ctxv\n",
    "import osmnx as ox\n",
    "from osmnx import graph_to_gdfs\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import MultiPoint\n",
    "import logging \n",
    "import pickle\n",
    "ox.settings.log_console = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../US_POI.csv\")\n",
    "df[\"SUB_CATEGORY\"] = df[\"SUB_CATEGORY\"].fillna(\"<null_val>\")\n",
    "df[\"TOP_CATEGORY\"] = df[\"TOP_CATEGORY\"].fillna(\"<null_val>\")\n",
    "df[\"LOCATION_NAME\"] = df[\"LOCATION_NAME\"].fillna(\"<null_val>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PLACEKEY', 'PARENT_PLACEKEY', 'LOCATION_NAME', 'SAFEGRAPH_BRAND_IDS',\n",
       "       'BRANDS', 'STORE_ID', 'TOP_CATEGORY', 'SUB_CATEGORY', 'NAICS_CODE',\n",
       "       'LATITUDE', 'LONGITUDE', 'STREET_ADDRESS', 'CITY', 'REGION',\n",
       "       'POSTAL_CODE', 'ISO_COUNTRY_CODE', 'PHONE_NUMBER', 'OPEN_HOURS',\n",
       "       'CATEGORY_TAGS', 'OPENED_ON', 'CLOSED_ON', 'TRACKING_CLOSED_SINCE',\n",
       "       'GEOMETRY_TYPE', 'DOMAINS', 'WEBSITE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  Hair, Nail, and Skin Care Services\n",
       "1                      Golf Courses and Country Clubs\n",
       "2    Offices of Physicians, Mental Health Specialists\n",
       "3                                   Investment Advice\n",
       "4                       Residential Property Managers\n",
       "Name: SUB_CATEGORY, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"SUB_CATEGORY\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_columns(row):\n",
    "    a = row[\"TOP_CATEGORY\"]\n",
    "    b = row[\"SUB_CATEGORY\"]\n",
    "    c = row[\"LOCATION_NAME\"]\n",
    "    # You can put any separator you like; here we just use a space or a special token:\n",
    "    return a + \"[sep]\" + b + \"[sep]\" + c\n",
    "\n",
    "# 2) Apply row-wise\n",
    "df[\"concatenated\"] = df.apply(concat_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\n",
    "    \"PLACEKEY\",\n",
    "    \"LONGITUDE\",      # “longtitude” in your prompt → correct column name is LONGITUDE\n",
    "    \"LATITUDE\",       # “lattitude” → correct column name is LATITUDE\n",
    "    \"concatenated\",\n",
    "    \"REGION\",\n",
    "    \"LOCATION_NAME\"\n",
    "]\n",
    "\n",
    "new_df = df[cols_to_keep]\n",
    "\n",
    "# (4) Save to CSV (without the index column):\n",
    "new_df.to_csv(\"poi_subset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_df = pd.read_csv(\"poi_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (new_df[\"LONGITUDE\"] >= -88.57)\n",
    "    & (new_df[\"LONGITUDE\"] <= 79.95)\n",
    "    & (new_df[\"LATITUDE\"] >= 24.45)\n",
    "    & (new_df[\"LATITUDE\"] <= 32.35)\n",
    ")\n",
    "FL_new_df = new_df[mask]\n",
    "FL_new_df.to_csv(\"Hex_bound_POI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# 2) Take just the “concatenated” column as a Python list of strings\n",
    "texts = new_df[\"concatenated\"].tolist()\n",
    "\n",
    "# 3) Build a simple HF Dataset with one field \"text\"\n",
    "tokenizer_ds = Dataset.from_dict({ \"text\": texts })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Load a pretrained tokenizer (e.g. BERT’s)\n",
    "#base_tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 5) Train-in-place on your “text” field\n",
    "\n",
    "#new_tok = base_tok.train_new_from_iterator(\n",
    "#    tokenizer_ds[\"text\"],                # an iterator over all concatenated strings\n",
    "#    vocab_size=base_tok.vocab_size + 5000,  # e.g. original BERT vocab + 5k new tokens\n",
    "#    show_progress=True\n",
    "#)\n",
    "\n",
    "# 6) Save the extended tokenizer\n",
    "#new_tok.save_pretrained(\"bert_extended_two_col_tok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1 new special token(s).\n",
      "['hair', ',', 'nail', ',', 'and', 'skin', 'care', 'services', '<null_val>']\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained(\"bert_extended_two_col_tok\")\n",
    "num_added = tok.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\"<null_val>\"]\n",
    "})\n",
    "print(f\"Added {num_added} new special token(s).\")\n",
    "print(tok.tokenize(\"Hair, Nail, and Skin Care Services <null_val>\"))\n",
    "#print(\"ID for <null_val>:\", tok.convert_tokens_to_ids(\"Hair, Nail, and Skin Care Services\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    LlamaTokenizer\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace \"hf_…\" with your actual Hugging Face API token\n",
    "login(\"hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test string: ['hair', ',', 'nail', ',', 'and', 'skin', 'care', 'services', '<null_val>']\n",
      "raw_ds (from concatenated column): 20309698 examples\n",
      "sample[0][:200]: Personal Care Services[sep]Hair, Nail, and Skin Care Services[sep]Troy Thomas ATL \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tok\n",
    "OUTPUT_DIR = \"./models/lora-3.2-1b-lm-finetuned\"\n",
    "print(\"Tokenizing test string:\",\n",
    "      tok.tokenize(\"Hair, Nail, and Skin Care Services <null_val>\"))\n",
    "\n",
    "# 2) Load the causal-LM model (Gemma-3-1b-it) and resize its embeddings\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-3-1b-it\",\n",
    "    attn_implementation=\"eager\"\n",
    ").to(device)\n",
    "model.resize_token_embeddings(len(tok))\n",
    "raw_ds = tokenizer_ds\n",
    "print(f\"raw_ds (from concatenated column): {len(raw_ds)} examples\")\n",
    "print(\"sample[0][:200]:\", raw_ds[0][\"text\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3TextScaledWordEmbedding(2487, 1152, padding_idx=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-3-1b-it\",\n",
    "    attn_implementation=\"eager\"\n",
    ").to(device)\n",
    "\n",
    "# (If you previously resized embeddings, make sure the tokenizer & model match:)\n",
    "model.resize_token_embeddings(len(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two texts <sep> are somewhat <sep> similar\n",
      " input_ids: [2, 620, 33, 56, 52, 1531, 62, 126, 11, 1428, 57, 12, 400, 46, 466, 60, 500, 43, 114, 11, 1428, 57, 12, 356, 3]\n",
      " tokens:    ['[CLS]', 'the', 't', '##w', '##o', 'te', '##x', '##ts', '<', 'se', '##p', '>', 'ar', '##e', 'so', '##m', '##ew', '##h', '##at', '<', 'se', '##p', '>', 'similar', '[SEP]']\n",
      "\n",
      "The two texts <sep> are somewhat <sep> not anyhow similar\n",
      " input_ids: [2, 620, 33, 56, 52, 1531, 62, 126, 11, 1428, 57, 12, 400, 46, 466, 60, 500, 43, 114, 11, 1428, 57, 12, 1577, 1791, 47, 43, 1071, 356, 3]\n",
      " tokens:    ['[CLS]', 'the', 't', '##w', '##o', 'te', '##x', '##ts', '<', 'se', '##p', '>', 'ar', '##e', 'so', '##m', '##ew', '##h', '##at', '<', 'se', '##p', '>', 'not', 'an', '##y', '##h', '##ow', 'similar', '[SEP]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "concatenated_texts = [\n",
    "    \"The two texts <sep> are somewhat <sep> similar\",\n",
    "    \"The two texts <sep> are somewhat <sep> not anyhow similar\"\n",
    "]\n",
    "\n",
    "for txt in concatenated_texts:\n",
    "    enc = tok(txt, return_tensors=\"pt\", truncation=True, padding=\"longest\")\n",
    "    print(txt)\n",
    "    print(\" input_ids:\", enc.input_ids[0].tolist())\n",
    "    print(\" tokens:   \", tok.convert_ids_to_tokens(enc.input_ids[0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch = tok(\n",
    "    concatenated_texts,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=256,      # Adjust as needed; 32 was very short.\n",
    "    padding=\"longest\",   # Pad to the longest sequence in the batch\n",
    "    return_attention_mask=True # Ensure attention_mask is returned\n",
    ").to(device)\n",
    "\n",
    "# Forward‐pass\n",
    "with torch.no_grad(): # Disable gradient calculations for inference\n",
    "    outputs = model(\n",
    "        input_ids=batch.input_ids,\n",
    "        attention_mask=batch.attention_mask,\n",
    "        output_hidden_states=True,  # You need this to access all hidden layers\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "# Get the hidden states from the last layer\n",
    "final_hidden = outputs.hidden_states[-1]  # shape: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "# --- Implement Mean Pooling (Recommended for Gemma sequence representation) ---\n",
    "attention_mask = batch.attention_mask # shape: (batch_size, seq_len)\n",
    "\n",
    "# Expand attention_mask to match the shape of token_embeddings for broadcasting\n",
    "mask_expanded = attention_mask.unsqueeze(-1).expand(final_hidden.size()).float()\n",
    "# shape: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "# Zero out the embeddings of padding tokens\n",
    "sum_embeddings = torch.sum(final_hidden * mask_expanded, dim=1)\n",
    "# shape: (batch_size, hidden_size)\n",
    "\n",
    "# Count the number of non-padding tokens in each sequence\n",
    "# Clamp to avoid division by zero for sequences that might be all padding (shouldn't happen with proper input)\n",
    "sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "# shape: (batch_size, hidden_size)\n",
    "\n",
    "# Compute the mean\n",
    "mean_pooled_embeddings = sum_embeddings / sum_mask\n",
    "# mean_pooled_embeddings shape: (batch_size, hidden_size)\n",
    "\n",
    "# Now mean_pooled_embeddings[i] is the single fixed-length vector for input sequence i.\n",
    "\n",
    "# Example usage for comparison (if you have at least two items in your batch)\n",
    "if mean_pooled_embeddings.shape[0] >= 2:\n",
    "    embedding_0 = mean_pooled_embeddings[0].cpu().numpy()\n",
    "    embedding_1 = mean_pooled_embeddings[1].cpu().numpy()\n",
    "\n",
    "    print(\"Length of each Mean Pooled embedding:\", len(embedding_0), len(embedding_1))\n",
    "    print(\"Mean Pooled embedding for first example (first 10 dims):\\n\", embedding_0[:10], \"…\\n\")\n",
    "    print(\"Mean Pooled embedding for second example (first 10 dims):\\n\", embedding_1[:10], \"…\\n\")\n",
    "\n",
    "    cos_sim_mean = torch.nn.functional.cosine_similarity(\n",
    "        mean_pooled_embeddings[0], mean_pooled_embeddings[1], dim=0\n",
    "    ).item()\n",
    "    print(f\"Cosine similarity (Mean Pooled embeddings) = {cos_sim_mean:.4f}\")\n",
    "\n",
    "elif mean_pooled_embeddings.shape[0] == 1:\n",
    "    embedding_0 = mean_pooled_embeddings[0].cpu().numpy()\n",
    "    print(\"Length of the single Mean Pooled embedding:\", len(embedding_0))\n",
    "    print(\"Mean Pooled embedding (first 10 dims):\\n\", embedding_0[:10], \"…\\n\")\n",
    "else:\n",
    "    print(\"Batch size is 0, no embeddings to display.\")\n",
    "\n",
    "# The 'mean_pooled_embeddings' tensor contains one fixed-length vector per input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c763885222754fb68880c6dfcfcf5d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20309698 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⮑ tok_ds: 20309698 examples\n",
      "  sample[0]: {'input_ids': [2, 190, 168, 111, 1, 1428, 57, 1, 852, 8], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_fn(examples):\n",
    "    return tok(examples[\"text\"], return_attention_mask=True)\n",
    "\n",
    "tok_ds = raw_ds.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "print(f\"⮑ tok_ds: {len(tok_ds)} examples\")\n",
    "print(\"  sample[0]:\", {k: tok_ds[0][k][:10] for k in (\"input_ids\", \"attention_mask\")}, \"\\n\")\n",
    "\n",
    "# 9) Group tokenized outputs into fixed‐length blocks (for causal LM)\n",
    "block_size = 64\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Flatten all input_ids and then split in chunks of size block_size\n",
    "    all_ids = sum(examples[\"input_ids\"], [])\n",
    "    total_length = (len(all_ids) // block_size) * block_size\n",
    "    all_ids = all_ids[:total_length]\n",
    "\n",
    "    chunks = [\n",
    "        all_ids[i : i + block_size]\n",
    "        for i in range(0, total_length, block_size)\n",
    "    ]\n",
    "    masks = [[1] * block_size for _ in chunks]\n",
    "    return {\n",
    "        \"input_ids\": chunks,\n",
    "        \"attention_mask\": masks,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397ab56252b4447a9964be0d0bd5f353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20309698 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⮑ lm_ds: 9013860 blocks\n",
      "\n",
      "  lm_ds[0]['input_ids'][:10]: [2, 190, 168, 111, 1, 1428, 57, 1, 852, 8] (len) 64\n"
     ]
    }
   ],
   "source": [
    "lm_ds = tok_ds.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1024,\n",
    "    num_proc=None,\n",
    "    remove_columns=tok_ds.column_names   # ← important!\n",
    ")\n",
    "print(f\"⮑ lm_ds: {len(lm_ds)} blocks\\n\")\n",
    "\n",
    "if len(lm_ds) == 0:\n",
    "    raise RuntimeError(\"No LM blocks were created—check block_size or your input data!\")\n",
    "else:\n",
    "    first_block = lm_ds[0][\"input_ids\"]\n",
    "    print(\"  lm_ds[0]['input_ids'][:10]:\", first_block[:10], \"(len)\", len(first_block))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./llm_ds.pickle\", \"wb\") as f:\n",
    "    pickle.dump(lm_ds,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./llm_ds.pickle\", \"rb\") as f:\n",
    "    lm_ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Set up LoRA for causal-LM fine-tuning\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tok,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# 13) Training arguments\n",
    "OUTPUT_DIR = \"./models/lora-3.2-1b-lm-finetuned-with-null\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None 0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fd5be990364c44bd084aed95e4d631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9013860 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 9013860\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"]\n",
    "    return examples\n",
    "\n",
    "# Apply this function to your dataset\n",
    "lm_ds = lm_ds.map(\n",
    "    add_labels,\n",
    "    batched=True,  # Process examples in batches\n",
    "    num_proc=None  # Set to your desired number of CPU cores for parallel processing\n",
    ")\n",
    "\n",
    "# Optional: Verify the dataset structure\n",
    "print(lm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Model device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/63493.tmpdir/ipykernel_52/3625868539.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='551' max='551' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [551/551 7:16:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>8.472500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.404300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.827500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.644800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.502100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.366700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.984900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.879900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.814100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.783200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.745600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.708200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.665100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.642100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.595700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.581600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.571300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.549700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.537100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.529300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.521700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.490900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.484600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.478900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.465200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.448700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.445500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.440200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.435700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.426500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.421900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.414700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=551, training_loss=2.943131866126225, metrics={'train_runtime': 26255.4215, 'train_samples_per_second': 343.314, 'train_steps_per_second': 0.021, 'total_flos': 2.4259644427832525e+18, 'train_loss': 2.943131866126225, 'epoch': 1.0})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "#model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "# 4. Apply torch.compile to the PEFT-wrapped model\n",
    "# This is the crucial step to enable compilation for speed, and it should be *after* PEFT\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoint-dir\",\n",
    "    per_device_train_batch_size=128*2,\n",
    "    gradient_accumulation_steps=64,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-4,\n",
    "    dataloader_num_workers=16,   # or 8 if you have spare CPU cores\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=30,\n",
    "    save_total_limit=3,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "#model_compiled = torch.compile(model) # Use a temporary variable first\n",
    "\n",
    "# 2. Then apply PEFT\n",
    "#model = get_peft_model(model_compiled, lora_cfg)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_ds,             # lm_ds still has columns \"input_ids\", \"attention_mask\"\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tok,             # or processing_class=tokenizer\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lm_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m lm_ds\n",
      "\u001b[31mNameError\u001b[39m: name 'lm_ds' is not defined"
     ]
    }
   ],
   "source": [
    "lm_ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
